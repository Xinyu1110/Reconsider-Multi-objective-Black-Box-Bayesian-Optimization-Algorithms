{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "015edb34",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## code for nqEVI, qEVI, qParGo, and random, completed based on botorch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61adde3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tkwargs = {\n",
    "    \"dtype\": torch.double,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from botorch.utils.transforms import unnormalize, normalize\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "\n",
    "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
    "from botorch.acquisition.objective import GenericMCObjective\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "#from botorch.utils.multi_objective.box_decomposition import NondominatedPartitioning\n",
    "%run botorch_utils_multi_objective_box_decomposition.ipynb\n",
    "#from botorch.acquisition.multi_objective.monte_carlo import qExpectedHypervolumeImprovement, qNoisyExpectedHypervolumeImprovement\n",
    "from botorch.acquisition.multi_objective.monte_carlo import qExpectedHypervolumeImprovement, qNoisyExpectedHypervolumeImprovement\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "\n",
    "from botorch import fit_gpytorch_model\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement, qNoisyExpectedImprovement\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "from botorch.utils.multi_objective.hypervolume import Hypervolume\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "%run MultiObjectives_version2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b23902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_data(num_points, problem):\n",
    "    # generate training data\n",
    "    lb = problem.lowerbounds\n",
    "    ub = problem.upperbounds\n",
    "    #print(lb)\n",
    "    bds = [lb, ub]\n",
    "    bds = torch.DoubleTensor(bds)\n",
    "    train_x = draw_sobol_samples(bounds=bds, n=1, q=num_points, seed=torch.randint(1000000, (1,)).item()).squeeze(0)\n",
    "    train_x_np = train_x.numpy()\n",
    "    train_obj_y = np.apply_along_axis(problem.MinMax_evaluate, 1, train_x_np)\n",
    "    train_obj = torch.from_numpy(train_obj_y)\n",
    "    #print(')))))',train_obj.shape)\n",
    "    return train_x, train_obj\n",
    "\n",
    "\n",
    "def initialize_model(train_x, train_obj):\n",
    "    # define models for objective and constraint\n",
    "    model = SingleTaskGP(train_x, train_obj, outcome_transform=Standardize(m=train_obj.shape[-1]))\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    return mll, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1010a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_qehvi_and_get_observation(model, train_obj, sampler, BATCH_SIZE, problem, standard_bounds):\n",
    "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    # partition non-dominated space into disjoint rectangles\n",
    "    partitioning = NondominatedPartitioning(ref_point = problem.ref_point, Y=train_obj)\n",
    "    acq_func = qExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=problem.ref_point.tolist(),  # use known reference point\n",
    "        partitioning=partitioning,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=20,\n",
    "        raw_samples=1024,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200, \"nonnegative\": True},\n",
    "        sequential=True,\n",
    "    )\n",
    "    # observe new values\n",
    "    bds = [problem.lowerbounds, problem.upperbounds]\n",
    "    bds = torch.DoubleTensor(bds)\n",
    "    new_x =  unnormalize(candidates.detach(), bounds=bds)\n",
    "\n",
    "    new_x_np = new_x.numpy()\n",
    "    new_obj_np = np.apply_along_axis(problem.MinMax_evaluate, 1, new_x_np)\n",
    "    new_obj = torch.from_numpy(new_obj_np)\n",
    "\n",
    "    # new_obj = problem(new_x)\n",
    "    return new_x, new_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a49dce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_qnehvi_and_get_observation(model, train_x, train_obj, sampler, BATCH_SIZE, problem, standard_bounds):\n",
    "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    # partition non-dominated space into disjoint rectangles\n",
    "    partitioning = NondominatedPartitioning(ref_point = problem.ref_point, Y=train_obj)\n",
    "    acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=problem.ref_point.tolist(),  # use known reference point \n",
    "        X_baseline=normalize(train_x, standard_bounds),\n",
    "        prune_baseline=True,  # prune baseline points that have estimated zero probability of being Pareto optimal\n",
    "        #partitioning=partitioning,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=20,\n",
    "        raw_samples=1024,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "        sequential=True,\n",
    "    )\n",
    "    \n",
    "    bds = [problem.lowerbounds, problem.upperbounds]\n",
    "    bds = torch.DoubleTensor(bds)\n",
    "    new_x =  unnormalize(candidates.detach(), bounds=bds)\n",
    "    \n",
    "    # observe new values \n",
    "    #new_x =  unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_x_np = new_x.numpy()\n",
    "    new_obj_np = np.apply_along_axis(problem.MinMax_evaluate, 1, new_x_np)\n",
    "    new_obj = torch.from_numpy(new_obj_np)\n",
    "    #new_obj_true = problem(new_x)\n",
    "    #new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
    "    return new_x, new_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "936f6c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_qparego_and_get_observation(model, train_obj, sampler, BATCH_SIZE, problem, standard_bounds):\n",
    "    \"\"\"Samples a set of random weights for each candidate in the batch, performs sequential greedy optimization\n",
    "    of the qParEGO acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    acq_func_list = []\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        weights = sample_simplex(problem.functiondim, **tkwargs).squeeze()\n",
    "        objective = GenericMCObjective(get_chebyshev_scalarization(weights=weights, Y=train_obj))\n",
    "        acq_func = qExpectedImprovement(  # pyre-ignore: [28]\n",
    "            model=model,\n",
    "            objective=objective,\n",
    "            best_f=objective(train_obj).max(),\n",
    "            sampler=sampler,\n",
    "        )\n",
    "        acq_func_list.append(acq_func)\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf_list(\n",
    "        acq_function_list=acq_func_list,\n",
    "        bounds=standard_bounds,\n",
    "        num_restarts=20,\n",
    "        raw_samples=1024,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "    )\n",
    "    # observe new values\n",
    "    bds = [problem.lowerbounds, problem.upperbounds]\n",
    "    bds = torch.DoubleTensor(bds)\n",
    "    new_x =  unnormalize(candidates.detach(), bounds=bds)\n",
    "\n",
    "    new_x_np = new_x.numpy()\n",
    "    new_obj_np = np.apply_along_axis(problem.MinMax_evaluate, 1, new_x_np)\n",
    "    new_obj = torch.from_numpy(new_obj_np)\n",
    "\n",
    "    # new_obj = problem(new_x)\n",
    "    return new_x, new_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baab3668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def botorch_qehi(N_TRIALS, N_BATCH, MC_SAMPLES, problem, initial_points, BATCH_SIZE, paths):\n",
    "\n",
    "    # BATCH_SIZE = 4\n",
    "    standard_bounds = torch.zeros(2, problem.dimension, **tkwargs)\n",
    "    standard_bounds[1] = 1\n",
    "\n",
    "    warnings.filterwarnings('ignore', category=BadInitialCandidatesWarning)\n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "    verbose = False\n",
    "\n",
    "    hvs_qehvi_all = []\n",
    "    hv = Hypervolume(ref_point=problem.ref_point)\n",
    "    obj_qehvi_all = []\n",
    "\n",
    "    # average over multiple trials\n",
    "    for trial in range(1, N_TRIALS + 1):\n",
    "        torch.manual_seed(trial)\n",
    "\n",
    "        print(f\"\\nTrial {trial:>2} of {N_TRIALS} \", end=\"\")\n",
    "        hvs_qehvi = []\n",
    "        #print(initial_points)\n",
    "\n",
    "        # call helper functions to generate initial training data and initialize model\n",
    "        train_x_qehvi, train_obj_qehvi = generate_initial_data(initial_points, problem)\n",
    "        #print(train_obj_qehvi)\n",
    "\n",
    "        # compute hypervolume\n",
    "        mll_qehvi, model_qehvi = initialize_model(train_x_qehvi, train_obj_qehvi)\n",
    "\n",
    "        # compute pareto front\n",
    "        pareto_mask = is_non_dominated(train_obj_qehvi)\n",
    "        pareto_y = train_obj_qehvi[pareto_mask]\n",
    "        print(pareto_y.shape[-1])\n",
    "\n",
    "        # compute hypervolume\n",
    "        volume = hv.compute(pareto_y)\n",
    "        hvs_qehvi.append(volume)\n",
    "\n",
    "        print(\n",
    "            f\"\\ninitial_points: Hypervolume (qEHVI) = \"\n",
    "            f\"({hvs_qehvi[-1]:>4.2f}), \", end=\"\")\n",
    "\n",
    "        # run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "        for iteration in range(1, N_BATCH + 1):\n",
    "            t0 = time.time()\n",
    "            # fit the models\n",
    "            fit_gpytorch_model(mll_qehvi)\n",
    "\n",
    "            # define the qEI and qNEI acquisition modules using a QMC sampler\n",
    "            qehvi_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n",
    "\n",
    "            # optimize acquisition functions and get new observations\n",
    "            new_x_qehvi, new_obj_qehvi = optimize_qehvi_and_get_observation(\n",
    "                model_qehvi, train_obj_qehvi, qehvi_sampler, BATCH_SIZE, problem, standard_bounds\n",
    "            )\n",
    "\n",
    "            # update training points\n",
    "            train_x_qehvi = torch.cat([train_x_qehvi, new_x_qehvi])\n",
    "            train_obj_qehvi = torch.cat([train_obj_qehvi, new_obj_qehvi])\n",
    "\n",
    "            # update progress\n",
    "            # compute pareto front\n",
    "            pareto_mask = is_non_dominated(train_obj_qehvi)\n",
    "            pareto_y = train_obj_qehvi[pareto_mask]\n",
    "            # compute hypervolume\n",
    "            volume = hv.compute(pareto_y)\n",
    "            hvs_qehvi.append(volume)\n",
    "\n",
    "            # reinitialize the models so they are ready for fitting on next iteration\n",
    "            # Note: we find improved performance from not warm starting the model hyperparameters\n",
    "            # using the hyperparameters from the previous iteration\n",
    "\n",
    "            t1 = time.time()\n",
    "\n",
    "            print(\n",
    "                f\"\\nBatch {iteration:>2}: Hypervolume (qEHVI) = \"\n",
    "                f\"({hvs_qehvi[-1]:>4.2f}), \"\n",
    "                f\"time = {t1 - t0:>4.2f}.\", end=\"\")\n",
    "\n",
    "            mll_qehvi, model_qehvi = initialize_model(train_x_qehvi, train_obj_qehvi)\n",
    "\n",
    "        obj_qehvi_all.append(train_obj_qehvi)\n",
    "        hvs_qehvi_all.append(hvs_qehvi)\n",
    "\n",
    "    ##write output\n",
    "    import os\n",
    "    output_dir = paths\n",
    "\n",
    "    if os.path.exists(os.path.join(output_dir, problem.name + '_qehi_front.txt')):\n",
    "        os.remove(os.path.join(output_dir, problem.name + '_qehi_front.txt'))\n",
    "\n",
    "    output_file = open(os.path.join(output_dir, problem.name + '_qehi_front.txt'), \"a\")\n",
    "    for n_trial_record in range(N_TRIALS):\n",
    "        output_file.write(\"num_trial: \" + str(n_trial_record) + '\\n')\n",
    "        obj_qehvi = obj_qehvi_all[n_trial_record].numpy()\n",
    "        for j in range(len(obj_qehvi)):\n",
    "            obj_j = obj_qehvi[j]\n",
    "            for xx in obj_j:\n",
    "                output_file.write(str(xx) + \" \")\n",
    "            output_file.write('\\n')\n",
    "    output_file.close()\n",
    "\n",
    "    if os.path.exists(os.path.join(output_dir, problem.name + '_qehi_hypervolume.txt')):\n",
    "        os.remove(os.path.join(output_dir, problem.name + '_qehi_hypervolume.txt'))\n",
    "\n",
    "    hv_file = open(os.path.join(output_dir, problem.name + '_qehi_hypervolume.txt'), \"a\")\n",
    "    for n_trial_record in range(N_TRIALS):\n",
    "        hv_file.write(\"num_trial: \" + str(n_trial_record) + '\\n')\n",
    "        hvs_qehvi = hvs_qehvi_all[n_trial_record]\n",
    "        for j in range(0, len(hvs_qehvi)):\n",
    "            hv_file.write(str(hvs_qehvi[j]) + \"\\n\")\n",
    "    hv_file.close()\n",
    "\n",
    "    return hvs_qehvi_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51686ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def botorch_nqehi(N_TRIALS, N_BATCH, MC_SAMPLES, problem, initial_points, BATCH_SIZE, paths):\n",
    "\n",
    "    # BATCH_SIZE = 4\n",
    "    standard_bounds = torch.zeros(2, problem.dimension, **tkwargs)\n",
    "    standard_bounds[1] = 1\n",
    "\n",
    "    warnings.filterwarnings('ignore', category=BadInitialCandidatesWarning)\n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "    verbose = False\n",
    "\n",
    "    hvs_nqehvi_all = []\n",
    "    hv = Hypervolume(ref_point=problem.ref_point)\n",
    "    obj_nqehvi_all = []\n",
    "\n",
    "    # average over multiple trials\n",
    "    for trial in range(1, N_TRIALS + 1):\n",
    "        torch.manual_seed(trial)\n",
    "\n",
    "        print(f\"\\nTrial {trial:>2} of {N_TRIALS} \", end=\"\")\n",
    "        hvs_nqehvi = []\n",
    "\n",
    "        # call helper functions to generate initial training data and initialize model\n",
    "        train_x_nqehvi, train_obj_nqehvi = generate_initial_data(initial_points, problem)\n",
    "\n",
    "        # compute hypervolume\n",
    "        mll_nqehvi, model_nqehvi = initialize_model(train_x_nqehvi, train_obj_nqehvi)\n",
    "\n",
    "        # compute pareto front\n",
    "        pareto_mask = is_non_dominated(train_obj_nqehvi)\n",
    "        pareto_y = train_obj_nqehvi[pareto_mask]\n",
    "\n",
    "        # compute hypervolume\n",
    "        volume = hv.compute(pareto_y)\n",
    "        hvs_nqehvi.append(volume)\n",
    "\n",
    "        print(\n",
    "            f\"\\ninitial_points: Hypervolume (nqEHVI) = \"\n",
    "            f\"({hvs_nqehvi[-1]:>4.2f}), \", end=\"\")\n",
    "\n",
    "        # run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "        for iteration in range(1, N_BATCH + 1):\n",
    "            t0 = time.time()\n",
    "            # fit the models\n",
    "            fit_gpytorch_model(mll_nqehvi)\n",
    "\n",
    "            # define the qEI and nqNEI acquisition modules using a QMC sampler\n",
    "            nqehvi_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n",
    "\n",
    "            # optimize acquisition functions and get new observations\n",
    "            new_x_nqehvi, new_obj_nqehvi = optimize_qnehvi_and_get_observation(\n",
    "                model_nqehvi, train_x_nqehvi, train_obj_nqehvi, nqehvi_sampler, BATCH_SIZE, problem, standard_bounds\n",
    "            )\n",
    "\n",
    "            # update training points\n",
    "            train_x_nqehvi = torch.cat([train_x_nqehvi, new_x_nqehvi])\n",
    "            train_obj_nqehvi = torch.cat([train_obj_nqehvi, new_obj_nqehvi])\n",
    "\n",
    "            # update progress\n",
    "            # compute pareto front\n",
    "            pareto_mask = is_non_dominated(train_obj_nqehvi)\n",
    "            pareto_y = train_obj_nqehvi[pareto_mask]\n",
    "            # compute hypervolume\n",
    "            volume = hv.compute(pareto_y)\n",
    "            hvs_nqehvi.append(volume)\n",
    "\n",
    "            # reinitialize the models so they are ready for fitting on next iteration\n",
    "            # Note: we find improved performance from not warm starting the model hyperparameters\n",
    "            # using the hyperparameters from the previous iteration\n",
    "\n",
    "            t1 = time.time()\n",
    "\n",
    "            print(\n",
    "                f\"\\nBatch {iteration:>2}: Hypervolume (nqEHVI) = \"\n",
    "                f\"({hvs_nqehvi[-1]:>4.2f}), \"\n",
    "                f\"time = {t1 - t0:>4.2f}.\", end=\"\")\n",
    "\n",
    "            mll_nqehvi, model_nqehvi = initialize_model(train_x_nqehvi, train_obj_nqehvi)\n",
    "\n",
    "        obj_nqehvi_all.append(train_obj_nqehvi)\n",
    "        hvs_nqehvi_all.append(hvs_nqehvi)\n",
    "\n",
    "    ##write output\n",
    "    import os\n",
    "    output_dir = paths\n",
    "\n",
    "    if os.path.exists(os.path.join(output_dir, problem.name + '_nqehi_front.txt')):\n",
    "        os.remove(os.path.join(output_dir, problem.name + '_nqehi_front.txt'))\n",
    "\n",
    "    output_file = open(os.path.join(output_dir, problem.name + '_nqehi_front.txt'), \"a\")\n",
    "    for n_trial_record in range(N_TRIALS):\n",
    "        output_file.write(\"num_trial: \" + str(n_trial_record) + '\\n')\n",
    "        obj_nqehvi = obj_nqehvi_all[n_trial_record].numpy()\n",
    "        for j in range(len(obj_nqehvi)):\n",
    "            obj_j = obj_nqehvi[j]\n",
    "            for xx in obj_j:\n",
    "                output_file.write(str(xx) + \" \")\n",
    "            output_file.write('\\n')\n",
    "    output_file.close()\n",
    "\n",
    "    if os.path.exists(os.path.join(output_dir, problem.name + '_nqehi_hypervolume.txt')):\n",
    "        os.remove(os.path.join(output_dir, problem.name + '_nqehi_hypervolume.txt'))\n",
    "\n",
    "    hv_file = open(os.path.join(output_dir, problem.name + '_nqehi_hypervolume.txt'), \"a\")\n",
    "    for n_trial_record in range(N_TRIALS):\n",
    "        hv_file.write(\"num_trial: \" + str(n_trial_record) + '\\n')\n",
    "        hvs_nqehvi = hvs_nqehvi_all[n_trial_record]\n",
    "        for j in range(0, len(hvs_nqehvi)):\n",
    "            hv_file.write(str(hvs_nqehvi[j]) + \"\\n\")\n",
    "    hv_file.close()\n",
    "\n",
    "    return hvs_nqehvi_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e12ae743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def botorch_qparego(N_TRIALS, N_BATCH, MC_SAMPLES, problem, initial_points, BATCH_SIZE, paths):\n",
    "\n",
    "    # BATCH_SIZE = 4\n",
    "    standard_bounds = torch.zeros(2, problem.dimension, **tkwargs)\n",
    "    standard_bounds[1] = 1\n",
    "\n",
    "    warnings.filterwarnings('ignore', category=BadInitialCandidatesWarning)\n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "    hvs_qparego_all = []\n",
    "    hv = Hypervolume(ref_point=problem.ref_point)\n",
    "    obj_qparego_all = []\n",
    "\n",
    "    # average over multiple trials\n",
    "    for trial in range(1, N_TRIALS + 1):\n",
    "        torch.manual_seed(trial)\n",
    "\n",
    "        print(f\"\\nTrial {trial:>2} of {N_TRIALS} \", end=\"\")\n",
    "        hvs_qparego = []\n",
    "\n",
    "        # call helper functions to generate initial training data and initialize model\n",
    "        train_x_qparego, train_obj_qparego = generate_initial_data(initial_points, problem)\n",
    "        mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_qparego)\n",
    "\n",
    "        # compute pareto front\n",
    "        pareto_mask = is_non_dominated(train_obj_qparego)\n",
    "        pareto_y = train_obj_qparego[pareto_mask]\n",
    "\n",
    "        # compute hypervolume\n",
    "        volume = hv.compute(pareto_y)\n",
    "        hvs_qparego.append(volume)\n",
    "        print(\n",
    "            f\"\\ninitial_points: Hypervolume (parego) = \"\n",
    "            f\"({hvs_qparego[-1]:>4.2f}), \", end=\"\")\n",
    "\n",
    "        # run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "        for iteration in range(1, N_BATCH + 1):\n",
    "            t0 = time.time()\n",
    "            # fit the models\n",
    "            fit_gpytorch_model(mll_qparego)\n",
    "\n",
    "            # define the qEI and qNEI acquisition modules using a QMC sampler\n",
    "            qparego_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n",
    "\n",
    "            # optimize acquisition functions and get new observations\n",
    "            new_x_qparego, new_obj_qparego = optimize_qparego_and_get_observation(\n",
    "                model_qparego, train_obj_qparego, qparego_sampler, BATCH_SIZE, problem, standard_bounds\n",
    "            )\n",
    "\n",
    "            # update training points\n",
    "            train_x_qparego = torch.cat([train_x_qparego, new_x_qparego])\n",
    "            train_obj_qparego = torch.cat([train_obj_qparego, new_obj_qparego])\n",
    "\n",
    "            # compute pareto front\n",
    "            pareto_mask = is_non_dominated(train_obj_qparego)\n",
    "            pareto_y = train_obj_qparego[pareto_mask]\n",
    "            # compute hypervolume\n",
    "            volume = hv.compute(pareto_y)\n",
    "            hvs_qparego.append(volume)\n",
    "\n",
    "            # reinitialize the models so they are ready for fitting on next iteration\n",
    "            # Note: we find improved performance from not warm starting the model hyperparameters\n",
    "            # using the hyperparameters from the previous iteration\n",
    "            mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_qparego)\n",
    "\n",
    "            t1 = time.time()\n",
    "\n",
    "            print(\n",
    "                f\"\\nBatch {iteration:>2}: Hypervolume (qparEGO) = \"\n",
    "                f\"({hvs_qparego[-1]:>4.2f}), \"\n",
    "                f\"time = {t1 - t0:>4.2f}.\", end=\"\")\n",
    "\n",
    "        obj_qparego_all.append(train_obj_qparego)\n",
    "        hvs_qparego_all.append(hvs_qparego)\n",
    "\n",
    "    ##write output\n",
    "    import os\n",
    "    output_dir = paths\n",
    "\n",
    "    if os.path.exists(os.path.join(output_dir, problem.name + '_qparEgo_front.txt')):\n",
    "        os.remove(os.path.join(output_dir, problem.name + '_qparEgo_front.txt'))\n",
    "\n",
    "    output_file = open(os.path.join(output_dir, problem.name + '_qparEgo_front.txt'), \"a\")\n",
    "    for n_trial_record in range(N_TRIALS):\n",
    "        output_file.write(\"num_trial: \" + str(n_trial_record) + '\\n')\n",
    "        obj_qparego = obj_qparego_all[n_trial_record].numpy()\n",
    "        for j in range(len(obj_qparego)):\n",
    "            obj_j = obj_qparego[j]\n",
    "            for xx in obj_j:\n",
    "                output_file.write(str(xx) + \" \")\n",
    "            output_file.write('\\n')\n",
    "    output_file.close()\n",
    "\n",
    "    if os.path.exists(os.path.join(output_dir, problem.name + '_qparEgo_hypervolume.txt')):\n",
    "        os.remove(os.path.join(output_dir, problem.name + '_qparEgo_hypervolume.txt'))\n",
    "\n",
    "    hv_file = open(os.path.join(output_dir, problem.name + '_qparEgo_hypervolume.txt'), \"a\")\n",
    "    for n_trial_record in range(N_TRIALS):\n",
    "        hv_file.write(\"num_trial: \" + str(n_trial_record) + '\\n')\n",
    "        hvs_qparego = hvs_qparego_all[n_trial_record]\n",
    "        for j in range(0, len(hvs_qparego)):\n",
    "            hv_file.write(str(hvs_qparego[j]) + \"\\n\")\n",
    "    hv_file.close()\n",
    "\n",
    "    return hvs_qparego_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9df337a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def botorch_random(N_TRIALS, N_BATCH, MC_SAMPLES, problem, initial_points, BATCH_SIZE, paths):\n",
    "\n",
    "    # BATCH_SIZE = 4\n",
    "    standard_bounds = torch.zeros(2, problem.dimension, **tkwargs)\n",
    "    standard_bounds[1] = 1\n",
    "\n",
    "    warnings.filterwarnings('ignore', category=BadInitialCandidatesWarning)\n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "    hvs_random_all = []\n",
    "    hv = Hypervolume(ref_point=problem.ref_point)\n",
    "    obj_random_all = []\n",
    "\n",
    "    # average over multiple trials\n",
    "    for trial in range(1, N_TRIALS + 1):\n",
    "        torch.manual_seed(trial)\n",
    "\n",
    "        print(f\"\\nTrial {trial:>2} of {N_TRIALS} \", end=\"\")\n",
    "        hvs_random = []\n",
    "\n",
    "        # call helper functions to generate initial training data and initialize model\n",
    "        train_x_random, train_obj_random = generate_initial_data(initial_points, problem)\n",
    "\n",
    "        # compute pareto front\n",
    "        pareto_mask = is_non_dominated(train_obj_random)\n",
    "        pareto_y = train_obj_random[pareto_mask]\n",
    "\n",
    "        # compute hypervolume\n",
    "        volume = hv.compute(pareto_y)\n",
    "        hvs_random.append(volume)\n",
    "\n",
    "        print(\n",
    "            f\"\\ninitial_points: Hypervolume (random) = \"\n",
    "            f\"({hvs_random[-1]:>4.2f}), \", end=\"\")\n",
    "\n",
    "        # run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "        for iteration in range(1, N_BATCH + 1):\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            new_x_random, new_obj_random = generate_initial_data(BATCH_SIZE, problem)\n",
    "\n",
    "            # update training points\n",
    "            train_x_random = torch.cat([train_x_random, new_x_random])\n",
    "            train_obj_random = torch.cat([train_obj_random, new_obj_random])\n",
    "\n",
    "            # update progress\n",
    "            # compute pareto front\n",
    "            pareto_mask = is_non_dominated(train_obj_random)\n",
    "            pareto_y = train_obj_random[pareto_mask]\n",
    "            # compute hypervolume\n",
    "            volume = hv.compute(pareto_y)\n",
    "            hvs_random.append(volume)\n",
    "\n",
    "\n",
    "            # reinitialize the models so they are ready for fitting on next iteration\n",
    "            # Note: we find improved performance from not warm starting the model hyperparameters\n",
    "            # using the hyperparameters from the previous iteration\n",
    "\n",
    "            t1 = time.time()\n",
    "\n",
    "            print(\n",
    "                f\"\\nBatch {iteration:>2}: Hypervolume (random) = \"\n",
    "                f\"({hvs_random[-1]:>4.2f}), \"\n",
    "                f\"time = {t1 - t0:>4.2f}.\", end=\"\")\n",
    "\n",
    "        obj_random_all.append(train_obj_random)\n",
    "        hvs_random_all.append(hvs_random)\n",
    "\n",
    "\n",
    "    ##write output\n",
    "    import os\n",
    "    output_dir = paths\n",
    "\n",
    "    if os.path.exists(os.path.join(output_dir, problem.name + '_random_front.txt')):\n",
    "        os.remove(os.path.join(output_dir, problem.name + '_random_front.txt'))\n",
    "\n",
    "    output_file = open(os.path.join(output_dir, problem.name + '_random_front.txt'), \"a\")\n",
    "    for n_trial_record in range(N_TRIALS):\n",
    "        output_file.write(\"num_trial: \" + str(n_trial_record) + '\\n')\n",
    "        obj_random = obj_random_all[n_trial_record].numpy()\n",
    "        for j in range(len(obj_random)):\n",
    "            obj_j = obj_random[j]\n",
    "            for xx in obj_j:\n",
    "                output_file.write(str(xx) + \" \")\n",
    "            output_file.write('\\n')\n",
    "\n",
    "    output_file.close()\n",
    "\n",
    "    if os.path.exists(os.path.join(output_dir, problem.name + '_random_hypervolume.txt')):\n",
    "        os.remove(os.path.join(output_dir, problem.name + '_random_hypervolume.txt'))\n",
    "\n",
    "    hv_file = open(os.path.join(output_dir, problem.name + '_random_hypervolume.txt'), \"a\")\n",
    "    for n_trial_record in range(N_TRIALS):\n",
    "        hv_file.write(\"num_trial: \" + str(n_trial_record) + '\\n')\n",
    "        hvs_random = hvs_random_all[n_trial_record]\n",
    "        for j in range(0, len(hvs_random)):\n",
    "            hv_file.write(str(hvs_random[j]) + \"\\n\")\n",
    "    hv_file.close()\n",
    "\n",
    "    return hvs_random_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa61647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bayes] *",
   "language": "python",
   "name": "conda-env-bayes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
